# Week 9: Watermarking LLM Outputs and AI-Generated Text Detection

## Learning Objectives

By the end of this lecture, students should be able to:

- Understand the AI-generated text detection problem and articulate why it is technically difficult
- Derive and analyze the Kirchenbauer et al. statistical watermarking scheme, including its detection test and failure modes
- Evaluate the robustness of watermarks against paraphrasing, spoofing, and scrubbing attacks
- Compare watermarking approaches against classifier-based and statistical detection methods on the dimensions of robustness, false positive rate, and computational cost

---

## 1. Motivation: The Detection Problem

### Why Detection Matters

The ability to distinguish AI-generated text from human-written text has become a significant concern across multiple domains:

- **Academic integrity**: students submitting AI-generated work as their own undermine the educational process; universities need reliable detection tools to enforce academic honesty policies
- **Election disinformation**: LLMs can generate convincing political content at scale; AI-generated propaganda, fake quotes attributed to candidates, and synthetic news articles are a demonstrated threat to democratic processes
- **Synthetic identity fraud**: AI-generated biographies, social media histories, and supporting documents enable the creation of convincing synthetic identities for financial fraud
- **Scientific paper mills**: LLMs can generate plausible-sounding scientific abstracts and papers, potentially flooding peer review systems with low-quality AI-generated submissions

**Scale is the core problem**: LLMs can produce thousands of words of convincing, contextually appropriate text in seconds, at near-zero marginal cost. This fundamentally changes the economics of content production for both legitimate and malicious purposes.

### The Core Technical Challenge

Detecting AI-generated text is fundamentally harder than detecting, for example, spam email. Spam has structural features (certain link patterns, sender characteristics, boilerplate phrases) that betray its origin. AI-generated text has no such inherent structural markers: it is drawn from the same natural language distribution as human text, generated by a model trained to predict human writing.

More formally: if an LLM perfectly modeled the human text distribution, no detection algorithm could distinguish its outputs from human writing at better than chance. In practice, LLMs are imperfect approximations of the human distribution, and their outputs exhibit subtle statistical artifacts — but these artifacts are small and become smaller as models improve.

### Taxonomy of Detection Approaches

There are three broad categories of AI text detection, each with distinct properties:

1. **Watermarking**: modify the generation process to embed a detectable statistical signal in the output. Requires control over the generating model. Detection is efficient and reliable when the watermark is present, but provides no signal for unwatermarked text.

2. **Statistical testing**: exploit statistical artifacts of the generation process (e.g., the tendency of LLM outputs to lie near log-probability maxima) without modifying generation. Works in zero-shot setting on any model but is computationally expensive and less reliable than watermarking.

3. **Classifier-based detection**: fine-tune a discriminative classifier on human vs. AI text. Simple and fast at inference time, but brittle: classifiers trained on outputs of one model degrade badly when applied to outputs of different models or newer model versions.

The remainder of this lecture covers each approach in detail, with emphasis on the watermarking approach because it offers the strongest formal guarantees.

---

## 2. Statistical Watermarking (Kirchenbauer et al. 2023)

### Algorithm Description

The Kirchenbauer et al. watermarking scheme is a token-level soft watermark that biases generation toward a pseudorandomly chosen subset of vocabulary tokens. The scheme works as follows:

**During generation**, at each time step $t$ when generating token $s_t$:

1. Compute a hash of the previous token: $h = H(s_{t-1}, k)$, where $H$ is a hash function and $k$ is a secret key known only to the detector.
2. Use $h$ to seed a pseudorandom number generator and partition the vocabulary $V$ into a "green list" $G$ and a "red list" $R$, where $|G| = \gamma |V|$ and $|R| = (1-\gamma)|V|$. Typically $\gamma = 0.5$, so the partition is equal.
3. Add a bias $\delta > 0$ to the logits of all green-list tokens before sampling:
$$\ell_i' = \begin{cases} \ell_i + \delta & \text{if token } i \in G \\ \ell_i & \text{if token } i \in R \end{cases}$$
4. Sample $s_t$ from the modified distribution $\text{softmax}(\ell')$.

The effect is that watermarked text will contain a higher-than-expected fraction of green tokens across the sequence.

**During detection**, given a candidate text $s = (s_1, s_2, \ldots, s_T)$ and the secret key $k$:

1. For each token $s_t$, reconstruct the green list $G_t$ using $H(s_{t-1}, k)$.
2. Count the number of green tokens: $|s_G| = \sum_{t=1}^T \mathbf{1}[s_t \in G_t]$.
3. Under the null hypothesis (no watermark), each token is independently drawn from the model's softmax distribution, unbiased toward green tokens. By the law of large numbers, the expected fraction of green tokens is $\mathbb{E}[|s_G|/T] = \gamma$.
4. Under the watermark, each token has an elevated probability of being a green token, so $\mathbb{E}[|s_G|/T] > \gamma$.
5. Compute the z-score:
$$z = \frac{|s_G| - \gamma T}{\sqrt{\gamma(1 - \gamma) T}}$$
6. Reject the null (classify as watermarked) if $z > z_\alpha$ for chosen significance level $\alpha$.

For $\gamma = 0.5$ and a sequence of $T = 200$ tokens, even a modest bias (e.g., 60% green tokens instead of 50%) produces a z-score of approximately $z = (120 - 100)/\sqrt{50} \approx 2.83$, corresponding to $p < 0.005$ — strong statistical evidence of watermarking.

### Properties of the Scheme

**Invisibility**: the logit bias $\delta$ shifts the output distribution slightly toward green tokens. For high-entropy positions (where the model assigns roughly uniform probability over many tokens), the shift is nearly imperceptible — the most probable token changes only if a green token has roughly equal logit to the top-ranked token. The effect on text quality is small for moderate $\delta$ (empirically, $\delta \approx 2$ provides strong watermark signal with minimal quality degradation).

**Efficiency**: detection requires only a single forward pass through the sequence (or no forward pass at all — just hash computations), making it $O(T)$ in the sequence length and computationally trivial.

**False positive rate**: for human-written text, the fraction of green tokens follows the null distribution with mean $\gamma$ and standard deviation $\sqrt{\gamma(1-\gamma)/T}$. The false positive rate is exactly $\alpha$ by construction of the z-test, assuming the null distribution is correctly specified. Empirically, Kirchenbauer et al. confirm that human text produces green token fractions consistent with the null.

**Low-entropy failure mode**: the watermark is effective only at positions where the model's distribution is high-entropy (the model is uncertain about the next token). At low-entropy positions — for example, generating the second token of "United States" after "United" — the model will generate the correct token regardless of whether it is green or red, because no alternative is plausible. These positions do not contribute to the watermark signal. In texts with many low-entropy positions (e.g., factual content with constrained phrasing), the effective number of watermarked tokens is much smaller than $T$, reducing the statistical power of the detection test.

---

## 3. Cryptographic Watermarking (Christ et al. 2023)

### Limitations of the Kirchenbauer Scheme

The Kirchenbauer scheme uses only the immediately preceding token $s_{t-1}$ to determine the green/red partition. This means an adversary who knows the scheme (but not the key $k$) can, in principle, infer the partitions from the generated text and exploit this to construct attacks. Additionally, the scheme provides only an informal notion of "invisibility" — the output distribution is modified, but there is no formal security proof bounding by how much.

### The Christ et al. Construction

Christ et al. 2023 propose a cryptographically stronger watermarking scheme with a formal undetectability guarantee. The key technical improvement is using a pseudorandom function $F_k$ keyed by the full prefix (all preceding tokens, not just the immediately preceding one) to determine the vocabulary partition:

$$h_t = F_k(s_1, s_2, \ldots, s_{t-1})$$

This makes the partition at each step a function of the entire generation history, which is computationally indistinguishable from a random partition for any observer without the key $k$ — even if the observer can observe both the text and the unmodified model.

**Formal guarantee**: the watermarked text distribution is computationally indistinguishable from the true model distribution for any probabilistic polynomial-time observer who does not know $k$. This is an **undetectability** property: no efficient algorithm can tell whether a given piece of text was watermarked, unless it has access to $k$. This is strictly stronger than Kirchenbauer et al.'s informal invisibility.

### Security Implications of Key Management

The Christ et al. scheme introduces a strong cryptographic dependency on the key $k$. Detection requires $k$; without it, detection is computationally infeasible. This creates a key management problem:

- If $k$ is held centrally (e.g., by the model provider), detection requires querying the provider — privacy implications for the submitted text
- If $k$ is made public, the undetectability guarantee is voided; an adversary who knows $k$ can construct adversarial attacks optimized against the specific partition function
- Key rotation complicates attribution: if $k$ changes, older watermarked text cannot be detected with the new key

This is a genuine engineering constraint that limits the practical deployment of cryptographic watermarking.

---

## 4. Robustness of Watermarks

Watermarks must be robust to post-processing attacks — modifications to the text that preserve its semantic content but disrupt the watermark signal. We analyze the main attack categories:

### Paraphrase Attacks

An adversary uses a second LLM (or human paraphraser) to rephrase the watermarked text, preserving meaning but changing surface form. Since the green/red partition is determined by specific tokens, paraphrasing that replaces tokens with synonyms will randomly change some green tokens to red and vice versa, degrading the z-score.

Kirchenbauer et al. report that aggressive paraphrasing using a dedicated paraphrase model reduces detection accuracy from approximately 99% to approximately 70% for a z-score threshold corresponding to 1% false positive rate. Less aggressive paraphrasing (e.g., replacing 20-30% of tokens) has a more modest effect. This suggests that paraphrasing is effective but not perfect — a motivated adversary who applies aggressive paraphrasing can significantly degrade the watermark.

A key defense is minimum text length: for short texts (under 50 tokens), the watermark signal is weak even before paraphrasing. Detection schemes should be deployed with explicit minimum length requirements, and short texts should be flagged as "insufficient evidence" rather than classified as non-watermarked.

### Copy-Paste Attacks

An adversary copies segments of watermarked text and interspersed them with non-watermarked text. The mixed document has a diluted z-score: if fraction $f$ of the document is watermarked and the rest is human-written, the expected z-score is approximately $f$ times the z-score of the fully watermarked text. For sufficiently small $f$, the z-score falls below the detection threshold.

Defense: localized detection (compute z-scores over sliding windows) can identify watermarked segments within otherwise unwatermarked documents.

### Spoofing Attacks

An adversary who knows the watermarking scheme (but not the key) cannot construct text that passes the detection test. However, an adversary who knows the key $k$ can embed the watermark in arbitrary text — including human-written text — to create a false attribution. This is a significant concern in adversarial settings: a malicious actor could "watermark" a human's writing to falsely accuse them of using AI.

Defense: the key should be treated as a secret. In the Christ et al. scheme, key knowledge is required to embed the watermark, so key secrecy prevents spoofing. In the Kirchenbauer scheme, the local dependence on only the previous token may make partial spoofing attacks feasible with careful analysis.

### Scrubbing Attacks

An adversary identifies green tokens in the watermarked text and replaces them with non-green synonyms. If the adversary knows the scheme and the key, this attack perfectly removes the watermark. If the adversary only knows the scheme but not the key, they cannot determine which tokens are green and which are red, so scrubbing would require random substitutions that degrade text quality.

The cryptographic watermarking scheme of Christ et al. is designed to make scrubbing computationally infeasible without the key: the partitions are pseudorandomly determined by the key, so an adversary without the key cannot identify green tokens.

---

## 5. Zero-Shot Detection Methods

Watermarking requires control over the generating model. When detecting text from an unknown or unmodified model, zero-shot statistical detection methods must be used.

### DetectGPT (Mitchell et al. 2023)

**Hypothesis**: AI-generated text tends to lie near local maxima of the generating model's log-probability function. Intuitively, sampling from an LLM with temperature produces text that has high probability under the model; minor perturbations of this text are likely to have lower probability. Human text, which is not generated by locally maximizing log-probability, does not exhibit this property as strongly.

**Algorithm**:
1. Compute the log-probability of the candidate text $x$ under the model: $\log p_\theta(x)$
2. Sample $k$ perturbations $\tilde{x}_1, \ldots, \tilde{x}_k$ of $x$ using a perturbation model (e.g., mask random spans and regenerate with T5)
3. Compute the perturbation discrepancy: $\Delta = \log p_\theta(x) - \frac{1}{k}\sum_{i=1}^k \log p_\theta(\tilde{x}_i)$
4. Classify as AI-generated if $\Delta > 0$ (the original has higher log-probability than its perturbations on average)

**Strengths**: no labeled training data required; works on any model for which log-probabilities are accessible; provides interpretable score.

**Weaknesses**: computationally expensive — requires $k$ forward passes through the detection model per candidate text, and $k$ regeneration passes through the perturbation model. For $k = 100$ and a large model, detection of a single document can take minutes. Also sensitive to paraphrasing: an aggressively paraphrased text may no longer lie near the original model's log-probability maximum.

### GLTR (Gehrmann et al. 2019)

GLTR ("Giant Language model Test Room") provides a visualization-based detection tool: for each token in the candidate text, compute its rank under the language model's predicted distribution (rank 1 = the most probable token). AI-generated text systematically uses higher-ranked (more probable) tokens than human text, which exhibits more varied and surprising word choices.

GLTR does not require training a classifier; it only requires access to a language model for computing token ranks. Its primary limitation is that the signals it exploits — high-rank token usage — are becoming less reliable as generation techniques improve and as higher-temperature sampling is used.

### Classifier-Based Detection

Fine-tune a discriminative classifier (e.g., RoBERTa) on a dataset of (text, label) pairs where labels indicate human-written vs. AI-generated. At inference time, classification is a single forward pass — very efficient.

**Strengths**: high accuracy when in-distribution (training and test text from same model).

**Weaknesses**:
- Brittle to distribution shift: classifiers trained on GPT-3 outputs perform poorly on GPT-4 outputs; classifiers trained on one domain (news) perform poorly in another (scientific text)
- Fail entirely for models not represented in training data
- Easy to defeat by stylistic modification: adding typos, changing register, or using less common vocabulary can fool classifiers
- **Fairness concern**: Liang et al. 2023 show that RoBERTa-based detectors misclassify non-native English speakers' writing as AI-generated at significantly higher rates than native speakers, because non-native writing uses simpler vocabulary and more common grammatical constructions that superficially resemble AI output

---

## 6. AI-Generated Image and Deepfake Detection

While this course focuses primarily on text, the detection problem extends to other modalities, and the techniques offer instructive parallels.

### GAN-Based Detection

Early deepfake detection work (e.g., FaceForensics++, Rossler et al. 2019) trained CNN classifiers to identify GAN-generated faces. GANs introduce distinctive artifacts in the frequency domain: GAN upsampling operations create characteristic checkerboard patterns in the high-frequency components of images. CNN detectors trained on these artifacts achieve near-perfect accuracy in-distribution.

The fundamental limitation: these artifacts are model-specific. Detectors trained on StyleGAN outputs fail on faces generated by different GAN architectures, and the artifacts become less pronounced with each successive GAN improvement. This is the classic arms-race dynamic.

### Diffusion Model Artifacts

Corvi et al. 2023 demonstrate that images generated by diffusion models have detectably different frequency statistics from both human-photographed images and GAN-generated images. Diffusion models tend to produce images with characteristic smoothness properties in the low-frequency components and specific noise patterns. Notably, JPEG compression — which selectively attenuates high-frequency components — affects diffusion-generated and real photographs differently, providing a detectable signal.

### Invisible Watermarks for Diffusion Models

**Tree-Ring Watermarks (Wen et al. 2023)**: rather than modifying the generated image post-hoc, Tree-Ring embeds the watermark in the initial noise vector used to start the diffusion process. The noise vector is chosen to have a specific pattern in Fourier space (concentric rings, hence the name). Because the diffusion process uses this noise throughout generation, the ring pattern is preserved in the final image in a distributed way that is robust to cropping, rotation, JPEG compression, and other common image transformations.

Detection requires only the secret key (to reconstruct the expected ring pattern), not the full diffusion model. Wen et al. demonstrate robustness to regeneration attacks (using a different diffusion model to partially regenerate the image) — a capability that no prior watermarking approach achieved.

**Stable Signature (Fernandez et al. 2023)**: fine-tunes the decoder component of a latent diffusion model to embed an imperceptible signature in all generated images. The signature is invisible to the human eye but detectable by a trained extractor network. The fine-tuning approach allows embedding a unique watermark per user (by fine-tuning a separate decoder per user), enabling attribution of generated images to specific accounts.

---

## 7. Open Problems and Policy Discussion

### Open Technical Problems

**Provable robustness to arbitrary paraphrasing**: can a watermarking scheme be designed that remains detectable after any semantic-preserving transformation of the text? Current answer: no. Zhao et al. 2023 make progress on this question but require strong assumptions about the paraphrase model. The fundamental difficulty is that "semantic-preserving" transformations can arbitrarily rearrange surface form, which is what the watermark is embedded in.

**The arms-race question**: is the relationship between watermarking and evasion attacks fundamentally adversarial (leading to an endless arms race), or can cryptographic-strength guarantees break the cycle? The Christ et al. framework suggests that cryptographic watermarking can provide undetectability guarantees that make certain attacks computationally infeasible. However, these guarantees hold only under computational complexity assumptions and assume the adversary does not have the detection key — conditions that may not hold in practice.

**Open-source models**: watermarking requires control over the generation process. Open-source LLMs (LLaMA, Mistral, etc.) can be run without any watermarking infrastructure. Any mandatory watermarking regime would need to address open-source models, which resist centralized control by design.

### Policy Context: EU AI Act Article 50

The EU AI Act (passed 2024) includes provisions requiring transparency about AI-generated content, including requirements that AI systems disclose when content is AI-generated and that generated content be watermarked where technically feasible. Article 50 specifically addresses this for general-purpose AI systems.

Key tensions this creates:

- **Feasibility**: the "technically feasible" qualifier is important — watermarking of open-source models is technically unfeasible at the generation stage. What are the obligations of open-source model providers?
- **Key management**: who holds the detection keys? A centralized key server enables efficient detection but creates a surveillance infrastructure. A distributed key model (providers hold their own keys) makes cross-provider detection difficult.
- **Accountability**: does watermarking enable attribution (this text was generated by model X), and if so, what legal liabilities does that create for model providers?

### Fairness Concerns

The documented misclassification of non-native English speakers as AI is not merely a technical imperfection — it is a fairness violation with real-world consequences. A student whose second-language English essay is flagged as AI-generated by an academic integrity tool faces an accusation that is difficult to disprove, and the accusation falls disproportionately on students from non-English-speaking backgrounds.

This concern applies to classifier-based detection. Watermarking is immune to this failure mode (it only detects text generated with the specific watermark), but watermarking cannot detect text from uncooperating or open-source models. In practice, both approaches will be deployed, and fairness constraints must be applied to the classifier-based approaches.

---

## 8. Discussion Questions

The following questions are intended for seminar discussion. Come prepared to engage with counterarguments.

1. **Is watermarking fundamentally a cryptographic problem or a machine learning problem?** The Christ et al. scheme provides cryptographic undetectability guarantees, but its robustness to paraphrasing depends on machine learning properties of the generating model. Can the two disciplines be cleanly separated, or is watermarking necessarily a hybrid problem?

2. **Should watermarking be mandatory for commercial LLMs? Who enforces it?** The EU AI Act moves in this direction. Consider: what enforcement mechanisms could work for cloud-based APIs vs. open-source models? What are the costs imposed on legitimate users by mandatory watermarking?

3. **How should academic integrity policies adapt to the limitations of AI detection?** Given that watermarking cannot detect open-source AI use, classifier-based detection has documented fairness problems, and paraphrasing can evade most schemes, should universities rely on AI detection tools at all? What alternative approaches to academic integrity are available?

4. **Is the false positive problem (misclassifying human text as AI) or the false negative problem (failing to detect AI text) more harmful?** Consider the downstream consequences for each error type in the context of academic integrity, election disinformation, and scientific publishing. How should detection thresholds be set given this asymmetry?

5. **Does tree-ring watermarking for images suggest an analogous approach for text?** The key insight in Tree-Ring is embedding the watermark in the seed of the generative process (the initial noise vector) rather than the output. For autoregressive LLMs, the "seed" is the initial state and random sampling choices. Can a similar approach — watermarking the sampling randomness rather than the logits — produce a more robust text watermark?

---

## 9. Key Papers

**Kirchenbauer et al. 2023 — "A Watermark for Large Language Models"**
The foundational paper for LLM watermarking, introducing the green/red list token-level watermark and the z-score detection test. Provides empirical evaluation of invisibility, robustness to paraphrasing, and statistical false positive rates; identifies the low-entropy token failure mode.

**Christ et al. 2023 — "Undetectable Watermarks for Language Models"**
Provides a cryptographically rigorous watermarking scheme with a formal undetectability guarantee: the watermarked distribution is computationally indistinguishable from the true model distribution for observers without the key. Strongest formal security guarantee in the watermarking literature.

**Mitchell et al. 2023 — "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature"**
Introduces the curvature-based detection hypothesis (AI text lies near log-probability maxima) and the DetectGPT algorithm. Demonstrates zero-shot detection capability across multiple LLMs and text domains; analyzes computational cost and robustness limitations.

**Gehrmann et al. 2019 — "GLTR: Statistical Detection and Visualization of Generated Text"**
Early work demonstrating that AI-generated text uses systematically higher-ranked vocabulary than human text, and that this pattern is visually and statistically detectable. Introduces a visualization tool that makes these patterns legible to human reviewers.

**Wen et al. 2023 — "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust"**
Proposes embedding watermarks in the initial noise vector of diffusion image generation in the Fourier domain, achieving robust watermarking that survives regeneration attacks, image cropping, and other transformations. Demonstrates that the diffusion process itself can serve as a watermark amplifier.

**Fernandez et al. 2023 — "The Stable Signature: Rooting Watermarks in Latent Diffusion Models"**
Introduces per-user watermarking of diffusion model outputs by fine-tuning the latent decoder to embed an imperceptible signature. Enables attribution of generated images to specific users or accounts, addressing accountability concerns in deployed systems.

**Rossler et al. 2019 — "FaceForensics++: Learning to Detect Manipulated Facial Images"**
Large-scale benchmark for face manipulation detection covering multiple GAN-based deepfake methods. Demonstrates that CNN classifiers can reliably detect GAN artifacts in-distribution; provides the canonical analysis of how detection accuracy degrades across manipulation methods.

**Corvi et al. 2023 — "On the Detection of Synthetic Images Generated by Diffusion Models"**
Analyzes the frequency-domain characteristics of diffusion-generated images and their differences from GAN-generated and photographic images. Shows that JPEG compression provides an additional discriminating signal; establishes that diffusion model detection requires different features than GAN detection.

**Zhao et al. 2023 — "Provable Robust Watermarking for AI-Generated Text"**
Addresses the robustness question formally, proposing a watermarking scheme with provable detection guarantees under bounded paraphrase attacks. Establishes theoretical conditions under which watermarks can survive semantic-preserving transformations and identifies the fundamental limits of provable robustness.
